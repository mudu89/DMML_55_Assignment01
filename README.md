
# End-to-End Data Management Pipeline for Machine Learning  
*Assignment I â€“ Data Management for Machine Learning (BITS Pilani)*  

---

## Project Overview
This project implements a complete **data management pipeline** for a **Telecom / Pay-TV churn prediction** use case.  
It covers all stages from **problem formulation â†’ ingestion â†’ validation â†’ preparation â†’ transformation â†’ feature store â†’ versioning â†’ modeling â†’ orchestration**.  

---

## Repository Structure

â”œâ”€â”€ README.md # Project index (this file)
â”œâ”€â”€ Task1_Problem_Formulation/
â”œâ”€â”€ Task2_DataIngestion/ # scripts + logs
â”œâ”€â”€ Task3_RawDataStorage/
â”œâ”€â”€ Task4_DataValidation/
â”œâ”€â”€ Task5_DataPreparation/
â”œâ”€â”€ Task6_DataTransformation/
â”œâ”€â”€ Task7_FeatureStore/
â”œâ”€â”€ Task8_DataVersioning/
â”œâ”€â”€ Task9_ModelBuilding/
â”œâ”€â”€ Task10_Orchestration/
â””â”€â”€ data/ # raw, validated, transformed datasets
---

## Deliverables by Task

### Task 1 â€“ Problem Formulation
- ğŸ“„ [Task1_Problem_Formulation/Task1_Problem_Formulation.md](Task1_Problem_Formulation.md)  
- Defines the **business problem, objectives, and data sources** for churn prediction in the Telecom/Pay-TV domain.  

### Task 2 â€“ Data Ingestion
- Scripts to fetch and log data ingestion from multiple sources (CSV, REST API).  
- Deliverables: ingestion scripts, logs, raw data snapshots.  

### Task 3 â€“ Raw Data Storage
- Data lake folder structure (partitioned by source/type/date).
- [Task3_RawDataStorage/Task3_RawDataStorage.md](Folder Structure)  
- Python scripts to upload/store ingested raw data.  

### Task 4 â€“ Data Validation
- Automated validation checks for missing values, anomalies, and schema mismatches.  
- Deliverables: validation scripts + sample data quality reports.  

### Task 5 â€“ Data Preparation
- Preprocessing pipeline: imputation, encoding, normalization, and EDA.  
- Deliverables: clean datasets + visualizations.  

### Task 6 â€“ Data Transformation
- Feature engineering & transformation logic.  
- Deliverables: transformed dataset, SQL schema, sample queries.  

### Task 7 â€“ Feature Store
- Centralized registry of features with metadata and versioning.  
- Deliverables: feature store configuration & retrieval examples.  

### Task 8 â€“ Data Versioning
- Data versioning strategy using DVC/Git.  
- Deliverables: repo showing dataset versions + documentation.  

### Task 9 â€“ Model Building
- Churn prediction models trained with prepared features.  
- Deliverables: training scripts, evaluation reports, versioned model files.  

### Task 10 â€“ Orchestration
- Automated DAG pipeline (Prefect/Airflow).  
- Deliverables: pipeline DAG, screenshots/logs of runs, monitoring setup.  

---

## How to Run
1. Clone this repo.  
2. Install dependencies (`requirements.txt`).  
3. Run task scripts individually (or orchestrate via Prefect/Airflow).  
4. Explore deliverables in each task folder.  

---

## ğŸ“½ï¸ Video Walkthrough
A short demo video (5â€“10 mins) will showcase the pipeline workflow and orchestration runs. *(to be added in final submission)*  

---
