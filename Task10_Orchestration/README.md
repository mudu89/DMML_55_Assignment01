# Task 10 - Pipeline Orchestration with Prefect

## ğŸ¯ Overview
This task implements the complete ML pipeline orchestration using **Prefect** for the DMML Assignment 01.

## ğŸ“ Files Structure
```
Task10_Orchestration/
â”œâ”€â”€ ml_pipeline.py          # Main pipeline with all 8 tasks
â”œâ”€â”€ start_pipeline.py       # Script to start Prefect server and run pipeline
â”œâ”€â”€ test_pipeline.py        # Test individual tasks
â””â”€â”€ README.md              # This file
```

## ğŸš€ Quick Start

### 1. Start the Prefect Server & Pipeline
```cmd
cd Task10_Orchestration
"C:/Users/msf12/OneDrive - Sky/Documents/BITS/Semester 2/DMML/DMML_55_Assignment01/.venv/Scripts/python.exe" start_pipeline.py
```

### 2. Monitor Pipeline
- **Prefect UI**: http://localhost:4200
- Watch tasks execute in real-time
- View logs and task dependencies

### 3. Run Pipeline Manually
```cmd
"C:/Users/msf12/OneDrive - Sky/Documents/BITS/Semester 2/DMML/DMML_55_Assignment01/.venv/Scripts/python.exe" ml_pipeline.py
```

## ğŸ“Š Pipeline Flow

```
Data Ingestion (Task 2)
    â†“
Raw Data Storage (Task 3)
    â†“
Data Validation (Task 4)
    â†“
Data Preparation (Task 5)
    â†“
Data Transformation (Task 6)
    â†“
Feature Store Update (Task 7)
    â†“
Data Versioning (Task 8)
    â†“
Model Building (Task 9)
```

## âœ… Current Status

- âœ… **Task 2 (Ingestion)**: Implemented and working
- ğŸš§ **Tasks 3-9**: Framework ready, scripts to be implemented
- âœ… **Task 10 (Orchestration)**: Complete Prefect pipeline setup

## ğŸ”§ Key Features

- **Error Handling**: Automatic retries with delays
- **Logging**: Comprehensive logging for each task
- **Dependencies**: Proper task sequencing with wait_for
- **Monitoring**: Real-time UI dashboard
- **Scalable**: Easy to add new tasks or modify existing ones

## ğŸ“¹ Demo Instructions

For your video demonstration:
1. Start the pipeline with `start_pipeline.py`
2. Show the Prefect UI at http://localhost:4200
3. Demonstrate task execution and monitoring
4. Show task logs and results

## ğŸ›  Next Steps

As you implement each task (3-9), simply update the corresponding function in `ml_pipeline.py` to call your actual scripts instead of the placeholder implementations.

Example for Task 3:
```python
@task(name="Raw Data Storage", retries=1)
def task_raw_data_storage():
    script_path = os.path.join(project_root, "Task3_RawDataStorage", "storage.py")
    result = subprocess.run([sys.executable, script_path], capture_output=True, text=True)
    # Handle result...
```
